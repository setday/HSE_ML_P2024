{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b97e03a4",
      "metadata": {
        "id": "b97e03a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/yvovaa/fork/HSE_ML_P2024\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "import torch\n",
        "class DQNPolicy:\n",
        "    def __init__(self, n_actions, station_space, n_hidden = 128, lr=0.005, path=None):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.dqn = torch.nn.Sequential(\n",
        "            torch.nn.Linear(station_space, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_actions)\n",
        "        )\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=0.001)\n",
        "        if path is not None:\n",
        "            self.dqn.load_state_dict(torch.load(path))\n",
        "\n",
        "    def make_action(self, state, eps=0):\n",
        "        with torch.no_grad():\n",
        "            best_action = torch.argmax(self.dqn(torch.Tensor(state)))\n",
        "            if torch.rand((1,)).item() > eps:\n",
        "                return best_action.item()\n",
        "            return torch.randint(self.n_actions, (1,)).item()\n",
        "\n",
        "\n",
        "    def update(self, state, next_state, action, reward, gamma=1):\n",
        "        q_values = self.dqn(torch.Tensor(state))\n",
        "        q_values_next = self.dqn(torch.Tensor(next_state))\n",
        "\n",
        "        q_values_should_be = self.dqn(torch.Tensor(state)).tolist().copy()\n",
        "        q_values_should_be[action] = reward + gamma*torch.max(q_values_next).item()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss(q_values, torch.Tensor(q_values_should_be)).backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f992b946",
      "metadata": {
        "id": "f992b946"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def learn_policy(env, n_episodes, policy, path, t_max=3000):\n",
        "    total_rewards = []\n",
        "\n",
        "    for _ in tqdm(range(n_episodes)):\n",
        "        state, _ = env.reset()\n",
        "        is_done = False\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        best_reward = -1e9\n",
        "        while not is_done and t < t_max:\n",
        "            action = policy.make_action(state, eps=0.1)\n",
        "            next_state, reward, is_done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            policy.update(state, next_state, action, reward)\n",
        "            state = next_state\n",
        "            t += 1\n",
        "        total_reward /= max(1, t)\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(policy.dqn.state_dict(), path)\n",
        "        total_rewards.append(total_reward)\n",
        "        clear_output(True)\n",
        "        plt.plot(total_rewards)\n",
        "        plt.show()\n",
        "    return total_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a05be015",
      "metadata": {
        "id": "a05be015"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import gym_game\n",
        "env = gym.make('Parkme')\n",
        "reset, _ = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8a88f1ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_actions = env.action_space.n\n",
        "states_dim = env.observation_space.shape[0]\n",
        "policy = DQNPolicy(n_actions, states_dim, path='models_bin/torch.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6fc66790",
      "metadata": {
        "id": "6fc66790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "  0%|          | 0/10 [00:24<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels_bin/torch.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mlearn_policy\u001b[0;34m(env, n_episodes, policy, path, t_max)\u001b[0m\n\u001b[1;32m     16\u001b[0m next_state, reward, is_done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 19\u001b[0m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     21\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mDQNPolicy.update\u001b[0;34m(self, state, next_state, action, reward, gamma)\u001b[0m\n\u001b[1;32m     30\u001b[0m q_values_should_be[action] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmax(q_values_next)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values_should_be\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3292\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3289\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3291\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "total_rewards = learn_policy(env, 10, policy, 'models_bin/torch.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebda659c",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(policy.dqn.state_dict(), 'models_bin/torch.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
