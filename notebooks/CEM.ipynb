{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "import gym\n",
    "import gym_game\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Parkme\")\n",
    "reset, _ = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "states_dim = env.observation_space.shape[0]\n",
    "print(n_actions, states_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_agent(path=None):\n",
    "    if path is None:\n",
    "        agent = MLPClassifier(\n",
    "            hidden_layer_sizes=(128),\n",
    "            activation=\"logistic\",\n",
    "        )\n",
    "        return agent.partial_fit(\n",
    "            [reset] * n_actions, range(n_actions), range(n_actions)\n",
    "        )\n",
    "    with open(path, \"rb\") as model:\n",
    "        return pickle.load(model)\n",
    "\n",
    "\n",
    "agent = load_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_session(env, agent, exploration_rate=0.8):\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "    try:\n",
    "        s, _ = env.reset()\n",
    "    except IndexError:\n",
    "        env.close()\n",
    "        del env\n",
    "        env = gym.make(\"Parkme\")\n",
    "        s, _ = env.reset()\n",
    "    for _ in range(3000):\n",
    "        if random.random() > exploration_rate:\n",
    "            action = np.random.choice(list(range(n_actions)))\n",
    "        else:\n",
    "            probs = agent.predict_proba([s])[0]\n",
    "            action = np.random.choice(list(range(n_actions)), p=probs)\n",
    "        new_s, r, terminated, _, _ = env.step(action)\n",
    "        states.append(s)\n",
    "        actions.append(action)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if terminated:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold:\n",
    "            elite_states.extend(states_batch[i])\n",
    "            elite_actions.extend(actions_batch[i])\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label=\"Mean rewards\")\n",
    "    plt.plot(list(zip(*log))[1], label=\"Reward thresholds\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines(\n",
    "        [np.percentile(rewards_batch, percentile)],\n",
    "        [0],\n",
    "        [100],\n",
    "        label=\"percentile\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models_bin/CEM.pkl\", \"wb\") as model:\n",
    "    pickle.dump(agent, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 85\n",
    "log = []\n",
    "n_epochs = 80\n",
    "best_reward = -1e12\n",
    "worst_reward = 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from joblib import parallel_backend\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    sessions = [generate_session(env, agent) for _ in range(100)]\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "    elite_states, elite_actions = select_elites(\n",
    "        states_batch, actions_batch, rewards_batch, percentile\n",
    "    )\n",
    "    # random_indices = np.random.choice(len(extra_elit_actions), 5000, replace=False)\n",
    "    # elite_states.extend(extra_elit_states[random_indices])\n",
    "    # elite_actions.extend(extra_elit_actions[random_indices])\n",
    "    with parallel_backend(\"loky\", n_jobs=-1):\n",
    "        agent = agent.partial_fit(elite_states, elite_actions)\n",
    "        agent = agent.partial_fit(extra_elit_states, extra_elit_actions)\n",
    "    mean = np.mean(rewards_batch)\n",
    "    if mean > best_reward:\n",
    "        with open(\"models_bin/CEM.pkl\", \"wb\") as model:\n",
    "            pickle.dump(agent, model)\n",
    "        best_reward = mean\n",
    "    worst_reward = min(worst_reward, mean)\n",
    "    show_progress(\n",
    "        rewards_batch, log, percentile, reward_range=[worst_reward, best_reward]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
